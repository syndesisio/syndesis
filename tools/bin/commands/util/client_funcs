#!/bin/bash

KUBE="kubectl"

source "$(basedir)/commands/util/openshift_funcs"
source "$(basedir)/commands/util/kube_funcs"

setup_client_binary() {
    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        #
        # Determine location of oc
        #
        setup_oc KUBE
    elif [ "${IS_OPENSHIFT}" == "NO" ]; then            #
        # Check for installation of kubectl
        #
        setup_kube KUBE
    else
        check_error "ERROR: Cannot determine client binary. Check you have a running cluster."
    fi
}

create_client_cache() {
    local share_dir=$(create_share_dir)
    #
    # Create the share directory for caching admin name
    #
    local dir="${share_dir}/client"
    if [ ! -d "${dir}" ]; then
        mkdir -p "${dir}"
    fi

    echo "${dir}"
}

get_current_user() {
    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        echo $(get_oc_user)
    else
        echo $(get_kube_user)
    fi
}

get_admin_user() {
    local admin="${1:-}"
    local client_dir=$(create_client_cache)
    local cache_file="${client_dir}/admin-${KUBE}-user"

    if [ -f "${cache_file}" ]; then
        local admin=$(head -n 1 "${cache_file}")
        echo ${admin}
        return
    fi

    if [ -z "${admin}" ]; then
        read -p "Please specify a cluster admin account to continue? : " admin
        if [ -z "${admin}" ]; then
            check_error "ERROR: Admin account not specified. Aborting."
        fi
    fi

    #
    # Cache the admin username in a file so we don't have to keep
    # asking the user. Doing it this way avoids running into problems
    # with global variables and subshells.
    #
    echo "${admin}" > ${cache_file}

    echo "${admin}"
}

login_as_admin() {
    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        echo $(login_oc_as_admin "${1:-}")
    else
        echo $(login_kube_as_admin "${1:-}")
    fi
}

is_cluster_admin() {
    ${KUBE} get clusterrolebindings > /dev/null 2>&1
    status=$?
    if [ ${status} -ne 0 ]; then
      echo "ERROR: Not successfully logged in as cluster-admin"
      return
    fi

    # Seems some implementations will allow the above
    ${KUBE} get namespaces > /dev/null 2>&1
    status=$?
    if [ ${status} -ne 0 ]; then
        echo "ERROR: Not successfully logged in as cluster-admin"
        return
    fi

    echo "OK"
}

get_current_namespace() {
    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        echo $(get_oc_project)
    else
        echo $(get_kube_namespace)
    fi
}

has_namespace() {
    if [ -z "${1:-}" ]; then
        check_error "ERROR: no namespace specified"
    fi

    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        status=$(get_oc_project "$1")
    else
        status=$(get_kube_namespace "$1")
    fi

    if [ "$(contains_error "${status}")" == "YES" ]; then
        echo "${status}"
    else
        echo "OK"
    fi
}

delete_namespace() {
    if [ -z "${1:-}" ]; then
        check_error "ERROR: no namespace specified"
    fi

    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        delete_oc_project "$1"
    else
        delete_kube_namespace "$1"
    fi
}

create_namespace() {
    if [ -z "${1:-}" ]; then
        check_error "ERROR: no namespace specified"
    fi

    local status=""
    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        status=$(create_oc_project "$1")
    else
        status=$(create_kube_namespace "$1")
    fi

    echo "${status}"
}

switch_namespace() {
    if [ -z "${1:-}" ]; then
        check_error "ERROR: no namespace specified"
    fi

    status=""
    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        status=$(switch_oc_project "$1")
    else
        status=$(switch_kube_namespace "$1")
    fi

    check_error "${status}"
    echo "${status}"
}

recreate_namespace() {
    local project="${1}"
    local dont_ask="${2:-false}"

    if [ -z "${project}" ]; then
        check_error "ERROR: No project given"
    fi

    # Delete project if existing
    if [ "$(has_namespace "${project}")" == "OK" ]; then
        if [ $dont_ask != "true" ]; then
            echo =============== WARNING -- Going to delete project ${project}
            ${KUBE} get all -n $project
            echo ============================================================
            read -p "Do you really want to delete the existing project $project ? yes/[no] : " choice
            echo
            if [ "${choice}" != "yes" ] && [ "${choice}" != "y" ]; then
                check_error "ERROR: Aborting on user's request"
            fi
        fi
        echo "Deleting project ${project}"
        delete_namespace "${project}"
    fi

    #
    # Wait for namespace to be properly removed
    #
    while ${KUBE} get namespace | grep "${project}"
    do
        sleep 3
    done

    # Create project afresh
    echo "Creating project ${project}"
    local status=$(create_namespace "${project}")
    check_error "${status}"
}

create_maven_mirror() {
    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        install_maven_mirror
    else
        echo "This option is not available in kubernetes."
    fi
}

get_custom_resource() {
    cr="${1:-}"

    if [ "${IS_OPENSHIFT}" == "YES" ]; then
        #
        # OS uses the internal default cr
        # or the one given on the cli
        #
        echo ${cr}
    else
        #
        # kube requires the cr generated with
        # a route hostname
        #
        if [ -n "${cr}" ]; then
            #
            # Prioritise the cli
            #
            echo ${cr}
            return
        fi

        if [ -f "$(basedir)/commands/share/kube-cr.yml" ]; then
            echo "$(basedir)/commands/share/kube-cr.yml"
        fi

        echo ""
    fi
}

scale_deployments() {
    local replicas=$1
    shift
    local dcs="$@"
    for dc in $dcs; do
        ${KUBE} scale dc $dc --replicas=$replicas
    done
    wait_for_deployments $replicas $dcs
}

wait_for_deployments() {
    local replicas_desired=$1
    shift
    local dcs="$@"

    ${KUBE} get pods -w &
    watch_pid=$!
    for dc in $dcs; do
        echo "Waiting for $dc to be scaled to ${replicas_desired}"
        local replicas=$(get_replicas $dc)
        while [ -z "$replicas" ] || [ "$replicas" -ne $replicas_desired ]; do
            echo "Sleeping 10s ..."
            sleep 10
            replicas=$(get_replicas $dc)
        done
    done
    kill $watch_pid
}

get_replicas() {
    local dc=${1}
    local hasDc=$(${KUBE} get deployment,dc -o name | ${GREP} $dc)
    if [ -z "$hasDc" ]; then
        echo "0"
        return
    fi
    ${KUBE} get "$hasDc" -o jsonpath="{.status.availableReplicas}"
}

pod() {
    local dc=${1}
    local ret=$(${KUBE} get pod -o name | ${GREP} "$dc" | sed -e "s/^pods\///")
    local nr_pods=$(echo $ret | wc -l | awk '$1=$1')
    if [ $nr_pods != "1" ]; then
        echo "ERROR: More than 1 pod found for $dc ($nr_pods found)"
    fi
    echo $ret
}

patch_pv_ref() {
    set +e
    local pv="${1:-}"
    if [ -z "${pv}" ]; then
        check_error "ERROR: Persistent volume reference not specified"
    fi

    ${KUBE} get pv/$pv | grep Released
    if [ "$?" == "1" ]; then
        echo "${pv} has not failed ... skipping"
        return
    fi

    echo "Patching ${pv} to remove claim reference"
    ${KUBE} patch pv/${pv} --type json -p '[{ "op": "remove", "path": "/spec/claimRef" }]'
    set -e
}

flush_pv_refs() {

    local revert_login=$(login_as_admin)

    pvs=$(${KUBE} get pv | grep -v NAME | awk {'print $1'})
    if [ $? -eq 0 ]; then
        for pv in ${pvs}
        do
            patch_pv_ref "${pv}"
        done
    fi

    $revert_login
}

#
# Shell into a pod
#
rsh() {
    local pod="${1:-}"
    if [ -z "${pod}" ]; then
        check_error "ERROR: Pod not specified."
    fi

    #
    # TODO consider if /bin/sh is sufficient
    #
    ${KUBE} exec -it "${pod}" -- "/bin/sh"
}

has_secret() {
    local secret="${1:-}"
    if [ -z "${secret}" ]; then
        check_error "ERROR: secret not specified."
    fi

    local result=$(${KUBE} get secret "${secret}" 2>&1)
    if [ "$(contains_error "${result}")" == "YES" ]; then
        echo "false"
    else
        echo "true"
    fi
}

delete_secret() {
    local secret="${1:-}"
    if [ -z "${secret}" ]; then
        check_error "ERROR: secret not specified."
    fi

    local result=$(${KUBE} delete secret "${secret}" 2>&1)
}

current_context() {
    echo $(${KUBE} config current-context)
}

get_cluster() {
    cur_ctx="$(current_context)" || { echo "ERROR: getting current context"; return; }
    cluster="$(${KUBECTL} config view -o=jsonpath="{.contexts[?(@.name==\"${cur_ctx}\")].context.cluster}")"

    if [[ -z "${cluster}" ]]; then
        echo "ERROR: Cannot find current kube cluster"
    else
        echo "${cluster}"
    fi
}

get_cluster_address() {
    cluster_name=$(get_cluster)
    if [ -z "${cluster_name}" ]; then
        check_error "ERROR: Cannot find current cluster"
    fi

    cluster="$(${KUBECTL} config view -o=jsonpath="{.clusters[?(@.name==\"${cluster_name}\")].cluster.server}")" \
        || check_error "ERROR: getting current cluster address"

    if [[ -z "${cluster}" ]]; then
        echo "Cannot find cluster address"
    else
        echo "${cluster}"
    fi
}
